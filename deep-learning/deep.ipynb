{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5ae1a12c7e4fa43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is the deep learning implementation of the model",
   "id": "52c072104121ef40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T08:24:38.077839Z",
     "start_time": "2025-03-01T08:24:37.828502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"../data-preparation/chat_emotions.csv\")\n",
    "display(df.head())\n"
   ],
   "id": "58695705a69fc54d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                text  emotion\n",
       "0  i cant do my kyc because nigerian banks are no...  sadness\n",
       "1  you don t need to add bank account to complete...  neutral\n",
       "2  verification will be automatic if you ve done ...  neutral\n",
       "3                    iam wait  hour not complete kyc  sadness\n",
       "4  it can take longer after working hour please s...  neutral"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i cant do my kyc because nigerian banks are no...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you don t need to add bank account to complete...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>verification will be automatic if you ve done ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iam wait  hour not complete kyc</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it can take longer after working hour please s...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "\n",
    "\n",
    "print(\"\\nData types:\\n\", df.dtypes)\n",
    "\n",
    "\n",
    "emotion_counts = df['emotion'].value_counts()\n",
    "print(\"\\nEmotion label distribution:\\n\", emotion_counts)\n",
    "\n",
    "\n",
    "emotion_percentage = df['emotion'].value_counts(normalize=True)\n",
    "print(\"\\nEmotion label percentage distribution:\\n\", emotion_percentage)\n",
    "\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing values:\\n\", missing_values)\n",
    "\n",
    "\n",
    "df['text_length'] = df['text'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "\n",
    "\n",
    "print(\"\\nText length statistics:\\n\", df['text_length'].describe())\n"
   ],
   "id": "4c8698b65897f731"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "num_duplicates = df.duplicated().sum()\n",
    "print(\"Number of duplicate rows:\", num_duplicates)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "\n",
    "df = df.dropna(subset=['emotion'])\n",
    "\n",
    "\n",
    "condition = (df['text_length'] >= 15) & (df['text_length'] <= 2000)\n",
    "num_outliers = df[~condition].shape[0]\n",
    "print(\"Number of outliers:\", num_outliers)\n",
    "df = df[condition]\n",
    "\n",
    "print(\"Shape of dataframe after cleaning:\", df.shape)\n"
   ],
   "id": "dbe920b4f4c1edc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "df['text_processed'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['emotion_encoded'] = label_encoder.fit_transform(df['emotion'])\n",
    "\n",
    "\n",
    "display(df[['text', 'text_processed', 'emotion', 'emotion_encoded']].head())\n"
   ],
   "id": "25a178c02ac69686"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='emotion', data=df)\n",
    "plt.title('Distribution of Emotion Labels')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "all_text = \" \".join(df['text_processed'])\n",
    "wordcloud = WordCloud(background_color='white', width=800, height=400).generate(all_text)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Most Frequent Words')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df['text_processed_length'] = df['text_processed'].apply(lambda x: len(x.split()))\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['text_processed_length'], bins=50, kde=True)\n",
    "plt.title('Distribution of Preprocessed Text Length')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ],
   "id": "ea1bfaaf6df7bb4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "df_balanced = df[['text_processed', 'emotion_encoded']]\n",
    "\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "\n",
    "X_resampled, y_resampled = ros.fit_resample(df_balanced[['text_processed']], df_balanced['emotion_encoded'])\n",
    "\n",
    "\n",
    "df_resampled = pd.concat(\n",
    "    [pd.DataFrame(X_resampled, columns=['text_processed']),\n",
    "     pd.Series(y_resampled, name='emotion_encoded')],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Resampled emotion distribution:\")\n",
    "print(df_resampled['emotion_encoded'].value_counts())\n"
   ],
   "id": "3a218783b58e175"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_dl = label_encoder.fit_transform(df['emotion'])"
   ],
   "id": "aac6b01b44970759"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # Added import\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Added import\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Prepare data\n",
    "texts = df['text_processed'].tolist()\n",
    "max_features = 20000  # Increased vocabulary size\n",
    "max_len = 150         # Increased sequence length\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_dl = label_encoder.fit_transform(df['emotion'])\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X_seq = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "X_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(\n",
    "    X_seq, y_dl, test_size=0.2, random_state=42, stratify=y_dl  # Reduced test size\n",
    ")\n",
    "X_train_dl, X_val_dl, y_train_dl, y_val_dl = train_test_split(\n",
    "    X_train_dl, y_train_dl, test_size=0.15, random_state=42, stratify=y_train_dl\n",
    ")\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_dl), y=y_train_dl)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Embedding(input_dim=max_features, output_dim=256, input_length=max_len),\n",
    "    layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv1D(64, 3, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2)\n",
    "\n",
    "\n",
    "history = model.fit(X_train_dl, y_train_dl,\n",
    "                    epochs=30,\n",
    "                    batch_size=64,  # Reduced batch size\n",
    "                    validation_data=(X_val_dl, y_val_dl),\n",
    "                    callbacks=[early_stop, lr_scheduler],\n",
    "                    class_weight=class_weights)\n",
    "\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test_dl, y_test_dl)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = model.predict(X_test_dl).argmax(axis=1)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_dl, y_pred, target_names=label_encoder.classes_))\n"
   ],
   "id": "21f2c1e824b76271"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
