{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is transformer implementation of the model"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n"
   ],
   "metadata": {
    "id": "esDFK_keyi9J"
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We assume thte data is loeaded and preprocessed and df is the dataframe\n",
    "we ignore the data loading and preprocessing steps becuase it is same with the traditional implementation"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['emotion_encoded'] = label_encoder.fit_transform(df['emotion'])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text_processed'], df['emotion_encoded'], test_size=0.2, random_state=42, stratify=df['emotion_encoded'])\n",
    "\n",
    "train_encodings = tokenizer(list(X_train), padding=True, truncation=True, max_length=128)\n",
    "test_encodings = tokenizer(list(X_test), padding=True, truncation=True, max_length=128)\n"
   ],
   "metadata": {
    "id": "8u3jyjOpyrWy"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['emotion_encoded'] = label_encoder.fit_transform(df['emotion'])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text_processed'], df['emotion_encoded'], test_size=0.2, random_state=42, stratify=df['emotion_encoded'])\n",
    "\n",
    "train_encodings = tokenizer(list(X_train), padding=True, truncation=True, max_length=128)\n",
    "test_encodings = tokenizer(list(X_test), padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Create a dataset class for BERT\n",
    "class EmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = EmotionDataset(train_encodings, y_train.values)\n",
    "test_dataset = EmotionDataset(test_encodings, y_test.values)\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",  # Updated argument\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"  # Disable WandB logging\n",
    ")\n",
    "\n",
    "# Define the Trainer and train the model\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the model to be trained\n",
    "    args=training_args,                  # training arguments\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,           # evaluation dataset\n",
    "    compute_metrics=lambda p: {\n",
    "        'accuracy': (np.array(p.predictions).argmax(axis=1) == np.array(p.label_ids)).mean()\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, preds, target_names=label_encoder.classes_))\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./emotion_bert_model')\n",
    "tokenizer.save_pretrained('./emotion_bert_model')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "scNArJEByu2D",
    "outputId": "f3cbfb4a-1278-493a-95b4-d719cc26b5b6"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4524' max='4524' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4524/4524 28:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.697800</td>\n",
       "      <td>0.819311</td>\n",
       "      <td>0.710378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.809700</td>\n",
       "      <td>0.759706</td>\n",
       "      <td>0.734914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.505400</td>\n",
       "      <td>0.808700</td>\n",
       "      <td>0.733588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.48      0.31      0.38       113\n",
      "     disgust       1.00      0.12      0.22         8\n",
      "        fear       0.44      0.29      0.35        56\n",
      "         joy       0.69      0.62      0.65       548\n",
      "     neutral       0.80      0.87      0.83      3676\n",
      "     sadness       0.61      0.62      0.62      1123\n",
      "    surprise       0.44      0.28      0.34       508\n",
      "\n",
      "    accuracy                           0.73      6032\n",
      "   macro avg       0.64      0.44      0.49      6032\n",
      "weighted avg       0.72      0.73      0.72      6032\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('./emotion_bert_model/tokenizer_config.json',\n",
       " './emotion_bert_model/special_tokens_map.json',\n",
       " './emotion_bert_model/vocab.txt',\n",
       " './emotion_bert_model/added_tokens.json')"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model and tokenizer to pickle files\n",
    "with open('emotion_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "with open('emotion_tokenizer.pkl', 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer, tokenizer_file)\n"
   ],
   "metadata": {
    "id": "54e7Ed5kzAjH"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the model using torch\n",
    "torch.save(model.state_dict(), 'emotion_model.pth')\n",
    "\n",
    "# Save the tokenizer using pickle (it's easier to load later)\n",
    "with open('emotion_tokenizer.pkl', 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer, tokenizer_file)\n"
   ],
   "metadata": {
    "id": "QYdtZmRa9bIQ"
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "hsxzlyJR9hIk"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
